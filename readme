# ğŸ§  Project 3A: Credit Card Fraud Detection with Imbalanced Data

Detecting fraudulent credit card transactions is a significant challenge in the financial world due to **highly imbalanced data** â€” fraudulent transactions are very rare compared to normal ones. This project aims to build a machine learning model capable of identifying fraud transactions with **high accuracy and low false alarms**.

---

## ğŸ“¦ Dataset
- Source: [Kaggle - Credit Card Fraud Detection](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud)
- Data size: 284,807 transactions
- Label: `Class` (0 = normal, 1 = fraud)

---

## ğŸ¯ Project Goals
- Handle **data imbalance (imbalanced classification)** using methods:
    - SMOTE
    - Undersampling
    - Class Weight
- Build and compare models:
    - Random Forest
    - XGBoost
    - LightGBM
- Perform **thorough evaluation** with metrics:
    - Precision
    - Recall
    - F1-score
    - ROC-AUC

---

## âš™ï¸ Tools & Technologies
- Python
- Scikit-learn
- Imbalanced-learn (SMOTE)
- XGBoost, LightGBM
- Matplotlib & Seaborn (visualization)
- Optuna (Hyperparameter tuning â€“ optional)

---

## ğŸ§ª Final Results & Insights

| Handling Method | Model           | Precision | Recall | F1-score | ROC AUC |
|-----------------|-----------------|-----------|--------|----------|---------|
| SMOTE           | Random Forest   | 0.83      | 0.83   | 0.83     | 0.97    |
| SMOTE           | XGBoost         | 0.77      | 0.87   | 0.82     | 0.97    |
| SMOTE           | LightGBM        | 0.60      | 0.86   | 0.71     | 0.97    |

ğŸ“Œ **SMOTE + Random Forest/XGBoost** provide the most balanced and stable results.

---

## ğŸ§  Additional Insights
- Undersampling yields high recall but very low precision (many false alarms).
- Class Weight is also quite good but less stable than SMOTE.
- After tuning, recall can be drastically improved, but caution is needed as precision can plummet â€” suitable for **high-risk cases** like fraud, where false alarms are preferable to missing actual fraud.

---
